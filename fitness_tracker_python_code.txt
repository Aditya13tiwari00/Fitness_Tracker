# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub

print('Data source import complete.')


# Import required libraries
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, mean_squared_error, accuracy_score
from sklearn.cluster import KMeans
from sklearn.neural_network import MLPClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# Load the dataset
file_path = '/kaggle/input/fitness-tracker-dataset/fitness_tracker_dataset.csv'
data = pd.read_csv(file_path)

data.head()

data.info()

data.describe()

# Step 1: Data Overview
# Check for missing values and data types
missing_values = data.isnull().sum()
data_types = data.dtypes
print("Missing Values:\n", missing_values)
print("\nData Types:\n", data_types)

# Visualize missing values
sns.heatmap(data.isnull(), cbar=False, cmap="viridis")
plt.title("Missing Values Heatmap")
plt.show()

import numpy as np

# Identify unique workout types (excluding missing values)
unique_workouts = data['workout_type'].dropna().unique()

# Fill missing values in workout_type randomly with one of the unique workout types
data['workout_type'] = data['workout_type'].apply(
    lambda x: np.random.choice(unique_workouts) if pd.isnull(x) else x
)

# Verify if all missing values are filled
missing_after = data['workout_type'].isnull().sum()
print("Missing values in 'workout_type' after filling:", missing_after)

# Visualize missing values
sns.heatmap(data.isnull(), cbar=False, cmap="viridis")
plt.title("Missing Values Heatmap")
plt.show()

# Step 3: Data Cleaning and Preprocessing
# Handle missing values in 'workout_type' by randomly filling them with existing activities
unique_workouts = data['workout_type'].dropna().unique()
data['workout_type'] = data['workout_type'].apply(
    lambda x: np.random.choice(unique_workouts) if pd.isnull(x) else x
)

# Verify all missing values are handled
print("Missing values after cleaning:\n", data.isnull().sum())

# Encode categorical variables and scale numerical variables
numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_cols = data.select_dtypes(include=['object']).columns.tolist()

numerical_imputer = SimpleImputer(strategy='mean')
categorical_imputer = SimpleImputer(strategy='most_frequent')
scaler = StandardScaler()
encoder = OneHotEncoder(handle_unknown='ignore')

preprocessor = ColumnTransformer(
    transformers=[
        ('num', Pipeline(steps=[('imputer', numerical_imputer), ('scaler', scaler)]), numerical_cols),
        ('cat', Pipeline(steps=[('imputer', categorical_imputer), ('encoder', encoder)]), categorical_cols)
    ]
)

# Apply preprocessing
processed_data = preprocessor.fit_transform(data)
from sklearn.decomposition import TruncatedSVD

# Use TruncatedSVD for dimensionality reduction
svd = TruncatedSVD(n_components=95)  # Adjust n_components based on your requirement
reduced_data = svd.fit_transform(processed_data)


# Step 4: Feature Engineering
# Create new features like activity intensity
data['calories_per_minute'] = data['calories_burned'] / data['active_minutes']
data['steps_per_km'] = data['steps'] / (data['distance_km'] + 1e-5)
data['sleep_efficiency'] = data['sleep_hours'] / (24 - data['active_minutes'] / 60)

# Step 5: Machine Learning Models
# Split data into train and test sets
X = reduced_data
y = data['mood']  # Example: Predict mood
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model 1: Logistic Regression
logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train, y_train)
y_pred_logreg = logreg.predict(X_test)
print("Logistic Regression Classification Report:\n", classification_report(y_test, y_pred_logreg))

# Model 2: Random Forest
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
print("Random Forest Classification Report:\n", classification_report(y_test, y_pred_rf))


# Model 3: Neural Network
nn = Sequential([
    Dense(128, activation='relu', input_dim=X_train.shape[1]),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dense(len(y.unique()), activation='softmax')
])

nn.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
early_stopping = EarlyStopping(monitor='val_loss', patience=5)

from sklearn.preprocessing import LabelEncoder

# Encode the labels into integers
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Check the mapping of labels
print("Label Mapping:", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))

# Use the encoded labels in model training
nn.fit(X_train, y_train_encoded, validation_data=(X_test, y_test_encoded), 
       epochs=50, callbacks=[early_stopping], batch_size=32)

# Step 6: Deep Learning Models
# Time-series analysis using LSTM (if applicable to dataset)
# Create a recurrent neural network for advanced trend detection (optional)

# Step 7: Novel Features for Smart Watches
# 1. Mood Predictor: Combine activity, sleep, and weather to suggest mood-enhancing activities.
# 2. Weather-Aware Recommendations: Suggest workouts based on current weather conditions.
# 3. Sleep Optimization Insights: Provide tailored recommendations for better sleep quality.
# 4. Personalized Workout Plans: Leverage activity intensity and calorie consumption data.


# Select only numeric columns
numeric_data = data.select_dtypes(include=['float64', 'int64'])

# Compute the correlation matrix
correlation_matrix = numeric_data.corr()

# Plot the heatmap
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()


# Use the encoded labels for evaluation
accuracies = [
    accuracy_score(y_test, y_pred_logreg),
    accuracy_score(y_test, y_pred_rf),
    nn.evaluate(X_test, y_test_encoded, verbose=0)[1]  # Use encoded labels
]

# Plot the accuracies
import matplotlib.pyplot as plt

models = ['Logistic Regression', 'Random Forest', 'Neural Network']
plt.bar(models, accuracies)
plt.title("Model Accuracy Comparison")
plt.xlabel("Model")
plt.ylabel("Accuracy")
plt.show()


import matplotlib.pyplot as plt
import seaborn as sns

# Set up figure for multiple plots
plt.figure(figsize=(15, 12))

# Plot distribution of numerical features
for i, col in enumerate(['user_id', 'date', 'steps', 'calories_burned', 
                         'distance_km', 'active_minutes', 
                         'sleep_hours', 'heart_rate_avg', 'workout_type', 
                         'weather_conditions','location','mood'], start=1):
    plt.subplot(4, 3, i)
    sns.histplot(data[col], kde=True)
    plt.title(f'Distribution of {col}')

plt.tight_layout()
plt.show()



